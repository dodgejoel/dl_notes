{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to try and explain my way through a simple backpropogation implementation. Let $\\sigma$ be the sigmoid function\n",
    "\n",
    "$\\displaystyle\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "We will consider a very simple single layer neural network that is predicting a membership in a single class. Our input data are in $\\mathbb{R}^n$ and so we have the following set up. \n",
    "\n",
    "$x \\in \\mathbb{R}^n$, $y \\in \\mathbb{R}$, $W \\in M_{n, 1}(\\mathbb{R})$, $b \\in \\mathbb{R}$. Where $x$ is a single input data, $y$ is a single target label (in $\\{0, 1\\}$), $W$ is the matrix of parameters for our predictor and b is our bias term. \n",
    "\n",
    "The form of our prediction function is then:\n",
    "\n",
    "$\\displaystyle F(x) = \\sigma(W\\cdot x + b)$\n",
    "\n",
    "Now fix a batch of input data $X = (x^1, x^2, \\dots, x^k)$ with labels $(y^1, y^2, \\dots, y^k)$. If we use cross entropy loss then our loss function over this batch of data is:\n",
    "\n",
    "$\\displaystyle J(W, b) = - \\sum_i y^i\\log(F(x^i))$\n",
    "\n",
    "(Maybe we actually take the average of this? It does not change the calculation as multiplying by a constant $k$ will just rescale every term in our calculations.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivatives\n",
    "\n",
    "Remember that our goal here is to compute $\\displaystyle \\frac{\\partial J}{\\partial w_i}$ for each $i = 1, \\dots, n$ and $\\displaystyle \\frac{\\partial J}{\\partial b}$.\n",
    "If we let \n",
    "\n",
    "$\\displaystyle f_W(x) = W \\cdot x + b$\n",
    "\n",
    "Then we can factor J nicely so that the chain rule tell us how to compute these derivatives.\n",
    "\n",
    "I will switch to prime notation for notational convenience. Since we just add up the loss terms for each element in $X$, we can go ahead and assume that $b = 1$ and write $x = x^1, y=y^1$. Then $J(W, b) = - y \\log(F_W(x))$.\n",
    "\n",
    "Now $\\displaystyle J' = (-y \\log (F_W(x)))' = -y \\cdot \\frac{1}{F_W(x)} \\cdot (F_W(x))'$ where we have used the chain rule in a straightforward way.\n",
    "\n",
    "We have factored $F_W(x) = \\sigma(f_W(x))$ so we can further unwrap the term\n",
    "\n",
    "$(F_W(x))' = \\sigma'(f_W(x)) \\cdot f_W'(x)$.\n",
    "\n",
    "Now depending on which variable we are using to take the derivative here (switching to prime notation has obscured this momentarily) we can easily compute $f_W'$.\n",
    "\n",
    "We have $f_W' = x_i$ if we are taking the derivative wrt $w_i$ and $f_W' = 1$ if we are taking the derivative wrt $b$. \n",
    "\n",
    "(For this maybe it is helpful to remember that $x \\in \\mathbb{R}^n$ and so $x = (x_1, \\dots, x_n)$.\n",
    "\n",
    "Putting this all together and wrangling the mixing of dimensions in all of these calculations, we get:\n",
    "\n",
    "$\\nabla J = -y \\cdot \\frac{1}{F_W(x)} \\cdot \\sigma'(f_W(x)) \\cdot (x_1, \\dots, x_n, 1)^T$\n",
    "\n",
    "We can clean this up a bit by writing $\\hat{y} = F_W(x)$ and recalling that $\\sigma'(z) = \\sigma(z) ( 1 - \\sigma(z))$. Also for convenience, let $\\tilde{x} = (x_1, \\dots, x_n, 1)$. Then we have \n",
    "\n",
    "$\\nabla J = \\frac{-y}{\\hat{y}} \\cdot \\hat{y} (1 - \\hat{y}) \\cdot \\tilde{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do not have batch size 1 then what happens?  Well in this case some of these operations are just vectorized and we sum down the batch axis at the end.\n",
    "\n",
    "$\\nabla J = \\displaystyle \\sum_i\\frac{-y^i}{\\hat{y^i}} \\cdot \\hat{y^i} (1 - \\hat{y^i}) \\cdot \\tilde{x^i}$ (or this divided by batch size for average loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Back prop\n",
    "I am not going to flush out exactly how general backprop works (the details still get kind of hairy for me :O) but it is worth pointing out how the terms in this finally gradient computation get rolled up into a more general multi layer computation.\n",
    "\n",
    "What we do is compute the gradient for the parameters in each layer one at a time and decompose the computation into several pieces, some of which have already been computed in our initial pass forward (needed to compute the loss initially!). The key is that the gradient for parameters in earlier layers depends on the gradient for parameters in later layers but these later gradients are independent of earlier layers. This means that we can get away with murder by computing the gradient for parameters in the deepest layers first and then incrementally step back to earlier layers. This is harder to see when there is only one layer :D.\n",
    "\n",
    "The computation of the gradient for the parameters in each layer is decomposed into three pieces:\n",
    "\n",
    "1. back propogated error terms from deeper in the network\n",
    "2. activation term at this layer\n",
    "3. linear term at this layer\n",
    "\n",
    "In our setup above we have:\n",
    "\n",
    "1. deeper errors (last layer) - $\\displaystyle \\frac {y}{\\hat{y}}$\n",
    "2. activation term at this layer - $\\hat{y}(1 - \\hat{y})$\n",
    "3. linear term at this layer - $\\tilde{x}$\n",
    "\n",
    "The book-keeping gets a little hairier but eh. . .not that bad probably.\n",
    "\n",
    "If we were to add another layer then we would have to compute a similar product of three terms but now term 1 would be replaced by this quantity that we are computing itself!\n",
    "\n",
    "Note that if we use a different activation function, the second term will change somewhat. Hopefully the activation function used is such that the derivative it is still very easy to express in terms of the value of the function itself (which we have already computed in forward prop). For relu for example this is the case as the derivative is just\n",
    "\n",
    "$ (\\text{relu})'(x) = 1 \\text{ if } x > 0 \\text{ else } 0$  ( i guess just blink and ignore the case when $x = 0$ :P\n",
    "\n",
    "If we use a different loss function then the initial error term that we roll backwards will be different but the way that we incrementally accumulate new terms as we move back through the layers will not change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
